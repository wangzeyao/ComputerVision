# 卷积神经网络



### 全链接前馈网络的问题
如果用全连接前馈网络来处理图像时，会存在以下两个问题：

 1. **参数太多**，如果输入图像大小为100×100×3，在全连接前馈网络中，第一个隐藏层的每个神
经元到输入层都有100×100×3 = 30, 000 个相互独立的连接，每个连接都对应一个权重参数。参数的规模也会急剧增加。这会导致整个神经网络的训练效率会非常低，也很容易出现过拟合。
 2. **局部不变性特征**，：自然图像中的物体都具有局部不变性特征，比如在尺度缩放、平移、旋转等操作不影响其语义信息。而全连接前馈网络很难提取这些局部不变特征，一般需要进行数据增强来提高性能。

## 1.卷积
### 1.1 二维卷积
给定一个图像$X \in \mathbb{R}^{M \times N}$，和滤波器$W \in \mathbb{R}^{m \times n}$，其卷积为：
$$ 
y_{i j}=\sum_{u=1}^{m} \sum_{v=1}^{n} w_{u v} \cdot x_{i-u+1, j-v+1}
 $$
 等于是把卷积核顺时针旋转180°后进行互相关
 ![enter image description here](https://lh3.googleusercontent.com/qHg0J9ywhH8i5ILb6D8Fp0xgBt42G8OEWes24Uu-oyJGRotc0jtgWAt7BPGe3oj-G5O8zONsts47)

在图像处理中，卷积经常作为特征提取的有效方法。一幅图像在经过卷积操作后得到结果称为**特征映射(Feature Map)**

### 1.2 互相关
给定一个图像$X \in \mathbb{R}^{M \times N}$，和滤波器$W \in \mathbb{R}^{m \times n}$，其互相关为：
$$ 
y_{i j}=\sum_{u=1}^{m} \sum_{v=1}^{n} w_{u v} \cdot x_{i+u-1, j+v-1}
 $$
互相关和卷积的区别在于卷积核仅仅是否进行翻转。因此互相关也可以称为不翻转卷积。

### 1.3 卷积的变种
在卷积的标准定义基础上，还可以引入滤波器的滑动步长和零填充来增加卷积的多样性，可以更灵活地进行特征抽取。

 - 步长(Stride)，是指滤波器在滑动时的时间间隔图给出了步长为2 的卷积示例。
 ![enter image description here](https://lh3.googleusercontent.com/tlyrmIi6JFG0eqgfK9lBxuuDiF5UMyjjGnznt-Baz-6SAoXJuQmM3zEw6xuwmXOVR2Cwha0__Ph9)
 -零填充（Zero Padding）是在输入向量两端进行补零。图给出了输入的两端各补一个零后的卷积示例。
 ![enter image description here](https://lh3.googleusercontent.com/rxP8f1-d5i8a9aAmpgZqVGM1Ew6bz6wS6vmvigJj35xkxvgPvG7B9bxasWmuUEDtVm-UbKDlv7oY)

假设卷积层的输入神经元个数为$n$，卷积大小为$m$，步长（stride）为$s$，输入神经元两端各填补$p$ 个零（zero padding），那么该卷积层的神经元数量为：
$$\frac{n-m+2p}{s+1}$$

一般常用的卷积有以下三类：

 

 - 窄卷积（Narrow Convolution）：步长$s = 1$，两端不补零$p = 0$，卷积后输出长度为$n − m + 1$。
 - 宽卷积（Wide Convolution）：步长$s = 1$，两端补零$p = m− 1$，卷积后输出长度$n + m − 1$。
 - 等宽卷积（Equal-Width Convolution）：步长$s = 1$，两端补零$p = (m−1)/2$，卷积后输出长度$n$。

### 1.4 卷积的数学性质
#### 1.4.1 交换性
即
$$ 
\mathbf{x} \otimes \mathbf{y}=\mathbf{y} \otimes \mathbf{x}
 $$
#### 1.4.2 导数
假设$Y=W \otimes X$，其中$X \in \mathbb{R}^{M \times N}, W \in \mathbb{R}^{m \times n}, Y \in \mathbb{R}^{(M-m+1) \times(N-n+1)}$，函数$f(Y) \in \mathbb{R}$为一个标量函数，则$$ 
\begin{aligned} \frac{\partial f(Y)}{\partial w_{u v}} &=\sum_{i=1}^{M-m+1} \sum_{j=1}^{N-n+1} \frac{\partial y_{i j}}{\partial w_{u v}} \frac{\partial f(Y)}{\partial y_{i j}} \\ &=\sum_{i=1}^{M-m+1} \sum_{j=1}^{N-n+1} x_{i+u-1, j+v-1} \frac{\partial f(Y)}{\partial y_{i j}} \\ &=\sum_{i=1}^{M-m+1} \sum_{j=1}^{N-n+1} \frac{\partial f(Y)}{\partial y_{i j}} x_{u+i-1, v+j-1} \end{aligned}
 $$
可以看出，$f(Y)$关于$W$的偏导数为$X$和$\frac{\partial f(Y)}{\partial Y}$的卷积
$$ 
\frac{\partial f(Y)}{\partial W}=\frac{\partial f(Y)}{\partial Y} \otimes X
 $$

## 2. 卷积神经网络
卷积神经网络一般由卷积层、汇聚层和全连接层构成。

### 2.1 用卷积来代替全连接
如果采用卷积来代替全连接，第$l$层的净输入$z^{(l)}$为第$l-1$层活性值(代入激活函数后的值？)$\mathbf{a}^{(l-1)}$和滤波器$\mathbf{w}^{(l)} \in \mathbb{R}^{m}$的卷积，即$$ 
\mathbf{z}^{(l)}=\mathbf{w}^{(l)} \otimes \mathbf{a}^{(l-1)}+b^{(l)}
 $$
 其中滤波器$\mathbf{w}^{(l)}$为可学习的权重向量，$b^{(l)} \in \mathbb{R}^{n^{l-1}}$为可学习的偏置。
 
 根据卷积的定义，卷积层有两个很重要的性质：
 

 - **局部连接**在卷积层（假设是第l 层）中的每一个神经元都只和下一层（第l − 1层）中某个局部窗口内的神经元相连，构成一个局部连接网络。如图所示，卷积层和下一层之间的连接数大大减少，由原来的$n^{l} \times n^{l-1}$个连接变为$n^{l} \times m$个连接，$m$为滤波器大小。
![enter image description here](https://lh3.googleusercontent.com/RTg1cmUA-Vf3YZvWQucQJ8ibeG1b_cvTZlmN-KPOd63exfHiKUD7i7gL-5ooxQVY-GOLhwErQ7aU)
 - **权重共享** 从公式$\mathbf{z}^{(l)}=\mathbf{w}^{(l)} \otimes \mathbf{a}^{(l-1)}+b^{(l)}$可以看出，作为参数的滤波器$\mathbf{w}^{(l)}$对于第$l$层的所有的神经元都是相同的。如上图b中，所有的同颜色连接上的权重都是一样的。
由于局部连接和权重共享，卷积层的参数只有一个$m$维的权重$\mathbf{w}^{(l)}$和$l$维的偏置$b^{(l)}$共$m + 1$个参数。参数个数和神经元的数量无关。此外，第$l$层的神经元个数不是任意选择的，而是满足$n^{(l)}=n^{(l-1)}-m+1$

### 2.2 卷积层
卷积层的作用是提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。
既然卷积网络主要应用在图像处理上，而图像为两维结构，因此为了更充分地利用图像的局部信息，通常将神经元组织为三维结构的神经层，其大小为高度M×宽度N×深度D，有D个M × N 大小的特征映射构成。
**特征映射**为一幅图像（或其它特征映射）在经过卷积提取到的特征，每个特征映射可以作为一类抽取的图像特征。为了提高卷积网络的表示能力，可以在每一层使用多个不同的特征映射，以更好地表示图像的特征。
在输入层，特征映射就是图像本身。如果是灰度图像，就是有一个特征映射，深度$D = 1$；如果是彩色图像，分别有RGB三个颜色通道的特征映射，输入层深度$D = 3$。
假设一个卷积层的结构如下：
* 输入特征映射组：$\mathbf{X} \in \mathbb{R}^{M \times N \times D}$为三维张量（tensor），其中每个切片(slice) 矩阵$X^{d} \in \mathbb{R}^{M \times N}$为一个输入特征映射,$1 \leq d \leq D$。
* 输出特征映射组：$\mathbf{Y} \in \mathbb{R}^{M^{\prime} \times N^{\prime} \times P}$为三维张量，其中每个切片矩阵$\mathbf{Y^{p}} \in \mathbb{R}^{M^{\prime} \times N^{\prime} }$为一个输出特征映射，$1 \leq p \leq P$。
* 卷积核：$\mathbf{W} \in \mathbb{R}^{m \times n \times D \times P}$为四维张量，其中每个切片矩阵$W^{p,d} \in \mathbb{R}^{m \times n}$为一个两维卷积核，$1 \leq d \leq D, 1 \leq p \leq P$。

![enter image description here](https://lh3.googleusercontent.com/Uh7TiP6LYdcz-kDlhazaNaKE3noAAypPyHdx_SuY6q1w5JaVmcVgX89aU04a6BjJmlvDQwBRlA4G)
另外还有一个例子：
![enter image description here](https://lh3.googleusercontent.com/n2ctdkCGDy0n-SLorQigGhAjXJHyTmk7xMgaGvZ0_Zx20yQQNeezHeJwR7c6DIc1wW8ATvLE3K27)
对应上面的式子，$M=7,N=7,D=3$，可以看到其实本来的输入是$5\times5$,但是加入了padding。
那么对于这样一个卷积层，我们有：
* 输入特征映射组：$\mathbf{X} \in \mathbb{R}^{7 \times 7 \times 3}$，每个切片(slice) 矩阵$X^{d} \in \mathbb{R}^{7 \times 7}$
* 卷积核：有$P=2$，$\mathbf{W} \in \mathbb{R}^{3 \times 3 \times 3 \times 2}$，为四维张量。
* 输出特征映射组：$\mathbf{Y} \in \mathbb{R}^{3 \times 3 \times 2}，$其中每个切片矩阵$\mathbf{Y^{p}} \in \mathbb{R}^{3 \times 3 }$为一个输出特征映射

**注意**这里是张量的卷积，即两个张量的3个子矩阵卷积后，再把卷积的结果相加后再加上偏倚b。

为了计算输出特征映射$Y^{p}$，用卷积核$W^{p, 1}, W^{p, 2}, \cdots, W^{p, D}$分别对输入特
征映射$X^{1}, X^{2}, \cdots, X^{D}$进行卷积，然后将卷积结果相加，并加上一个标量偏置$b$得到卷积层的净输入$Z^{p}$，再经过非线性激活函数后得到输出特征映射$Y^{p}$
$$ 
\begin{aligned} Z^{p} &=\mathbf{W}^{p} \otimes \mathbf{X}+b^{p}=\sum_{d=1}^{D} W^{p, d} \otimes X^{d}+b^{p} \\ Y^{p} &=f\left(Z^{p}\right) \end{aligned}
 $$
其中$f(\cdot)$为非线性激活函数。
所以，在输入为$\mathbf{X} \in \mathbb{R}^{M \times N \times D}$，输出为$\mathbf{Y} \in \mathbb{R}^{M^{\prime} \times N^{\prime} \times P}$的卷积层中，每一个输入特征映射都需$D$个滤波器以及一个bias偏置。假设每个滤波器的大小为$m \times n$，那么共需要$P \times D \times(m \times n)+P$个参数

### 2.3 汇聚层 Pooling Layer
其作用是进行特征选择，降低特征数量，并从而减少参数数量。

卷积层虽然可以显著减少网络中连接的数量，但特征映射组中的神经元个数并没有显著减少。如果后面接一个分类器，分类器的输入维数依然很高，很
容易出现过拟合。为了解决这个问题，可以在卷积层之后加上一个汇聚层，从而降低特征维数，避免过拟合。

假设汇聚层的输入特征映射组为$\mathbf{X} \in \mathbb{R}^{M \times N \times D}$，对于其中每一个特征映射$X^{d}$，将其划分为很多区域$R_{m, n}^{d}, 1 \leq m \leq M^{\prime}, 1 \leq n \leq N^{\prime}$这些区域可以重叠，也可以不重叠。汇聚(Pooling) 是指对每个区域进行下采样（Down Sampling）得到一个值，作为这个区域的概括。

常用的汇聚函数有两种
* 最大汇聚（Maximum Pooling）：一般是取一个区域内所有神经元的最大值。$$ 
Y_{m, n}^{d}=\max _{i \in R_{m, n}^{d}} x_{i}
 $$
 其中$x_{i}$为区域$R_{k}^{d}$内每个神经元的激活值。
* 平均汇聚（Mean Pooling）：一般是取区域内所有神经元的平均值。$$ 
Y_{m, n}^{d}=\frac{1}{\left|R_{m, n}^{d}\right|} \sum_{i \in R_{m, n}^{d}} x_{i}
 $$

图5.8给出了采样最大汇聚进行子采样操作的示例。可以看出，汇聚层不但
可以有效地减少神经元的数量，还可以使得网络对一些小的局部形态改变保持
不变性，并拥有更大的感受野。
![enter image description here](https://lh3.googleusercontent.com/j6myEpoY4b5cVa7_LL-Ddmal-nySFhnJ-3K0hhL5tD7tuMkdLj9w-at_glTRsPZYc9BrgRDAUnu3)

### 2.4 典型的卷积网络结构
一个典型的卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成。目前常用的卷积网络结构图5.9所示。一个卷积块为连续$M$个卷积层和$b$ 个汇聚层（$M$通常设置为$2 ∼ 5$，$b$ 为$0$ 或$1$）。一个卷积网络中可以堆叠$N$个连续的卷积块，然后在接着$K$个全连接层（$N$ 的取值区间比较大，比如$1 ∼ 100$或者更大；$K$一般为$0 ∼ 2$）。
![enter image description here](https://lh3.googleusercontent.com/Np1Ly8xMd-d8lMC1dWlW0tDCtO2FnVg9XdP2HP2CbGXchS7Fth0qb-O8-L7odxvw32lD1k0YOdn4)

## 3. 卷积神经网络的反向传播
### 3.1 已知卷积层的$\delta^{l}$，推导上一隐藏层的$\delta^{l-1}$
我们知道$\delta^{l}$和$\delta^{l-1}$的递推公式为
$$ 
\delta^{l-1}=\frac{\partial C}{\partial z^{l-1}}=\frac{\partial C}{\partial z^{l}} \frac{\partial z^{l}}{\partial z^{l-1}}=\delta^{l} \frac{\partial z^{l}}{\partial z^{l-1}}
 $$
因此要推导出$\delta^{l}$和$\delta^{l-1}$的关系，需要知道$\frac{\partial z^{l}}{\partial z^{l-1}}$。

我们有$z^{l}$和$z^{l-1}$的关系：
$$ 
z^{l}=a^{l-1} * W^{l}+b^{l}=\sigma\left(z^{l-1}\right) * W^{l}+b^{l}
 $$
因此我们有：
$$ 
\delta^{l-1}=\delta^{l} \frac{\partial z^{l}}{\partial z^{l-1}}=\delta^{l} * r o t 180\left(W^{l}\right) \odot \sigma^{\prime}\left(z^{l-1}\right)
 $$
这里的式子其实和DNN的类似，区别在于对于含有卷积的式子求导时，卷积核被旋转了180度。即式子中的$\operatorname{rot} 180( )$，翻转180度的意思是上下翻转一次，接着左右翻转一次。

假设我们$l-1$层的输出$a^{l-1}$是一个$3 \times 3$矩阵，第$l$层的卷积核$W^{l}$是一个$2 \times 2$矩阵，采用1像素的步幅，则输出$z^{l}$是一个$2 \times 2$的矩阵。我们简化$b^{l}$都是都是0,则有
$$ 
a^{l-1} * W^{l}=z^{l}
 $$
矩阵表达式如下：
$$ 
\left( \begin{array}{lll}{a_{11}} & {a_{12}} & {a_{13}} \\ {a_{21}} & {a_{22}} & {a_{23}} \\ {a_{31}} & {a_{32}} & {a_{33}}\end{array}\right) * \left( \begin{array}{cc}{w_{11}} & {w_{12}} \\ {w_{21}} & {w_{22}}\end{array}\right)=\left( \begin{array}{cc}{z_{11}} & {z_{12}} \\ {z_{21}} & {z_{22}}\end{array}\right)
 $$
利用卷积的定义，很容易得出：
$$ 
\begin{array}{l}{z_{11}=a_{11} w_{11}+a_{12} w_{12}+a_{21} w_{21}+a_{22} w_{22}} \\ {z_{12}=a_{12} w_{11}+a_{13} w_{12}+a_{22} w_{21}+a_{23} w_{22}} \\ {z_{21}=a_{21} w_{11}+a_{22} w_{12}+a_{31} w_{21}+a_{32} w_{22}} \\ {z_{22}=a_{22} w_{11}+a_{23} w_{12}+a_{32} w_{21}+a_{33} w_{22}}\end{array}
 $$
我们有：
![enter image description here](https://lh3.googleusercontent.com/Yv3nTa1GSqrg2xaahGLaMUiia2M5-tjqwBIxadzXgdNfU3izvs2VK3Qg97T5Ll1049YELXhRSkP3)
$$ 
\nabla a^{l-1}=\frac{\partial J(W, b)}{\partial a^{l-1}}=\frac{\partial J(W, b)}{\partial z^{l}} \frac{\partial z^{l}}{\partial a^{l-1}}=\delta^{l} \frac{\partial z^{l}}{\partial a^{l-1}}
 $$
对于$a_{11}$的梯度，由于在4个等式中$z_{11}$有乘积关系，从而我们有：
$$ 
\nabla a_{11}=\delta^{l}_{11} w_{11}
 $$
 同理得：
$$ 
\nabla a_{12}=\delta^{l}_{11} w_{12}+\delta^{l}_{12} w_{11}
 $$
 和
 $$ 
\begin{array}{c}{\nabla a_{13}=\delta_{12} w_{12}} \\ {\nabla a_{21}=\delta_{11} w_{21}+\delta_{21} w_{11}} \\ {\nabla a_{22}=\delta_{11} w_{22}+\delta_{12} w_{21}+\delta_{21} w_{12}+\delta_{22} w_{11}} \\ {\nabla a_{23}=\delta_{12} w_{22}+\delta_{22} w_{12}} \\ {\nabla a_{32}=\delta_{21} w_{22}+\delta_{22} w_{21}} \\ {\nabla a_{32}=\delta_{21} w_{22}}\end{array}
 $$
这上面9个式子其实可以用一个矩阵卷积的形式表示，即：
$$ 
\left( \begin{array}{cccc}{0} & {0} & {0} & {0} \\ {0} & {\delta_{11}} & {\delta_{12}} & {0} \\ {0} & {\delta_{21}} & {\delta_{22}} & {0} \\ {0} & {0} & {0} & {0}\end{array}\right) * \left( \begin{array}{cc}{w_{22}} & {w_{21}} \\ {w_{12}} & {w_{11}}\end{array}\right)=\left( \begin{array}{ccc}{\nabla a_{11}} & {\nabla a_{12}} & {\nabla a_{13}} \\ {\nabla a_{21}} & {\nabla a_{22}} & {\nabla a_{23}} \\ {\nabla a_{31}} & {\nabla a_{32}} & {\nabla a_{33}}\end{array}\right)
 $$
为了符合梯度计算，我们在误差矩阵周围填充了一圈0，此时我们将卷积核翻转后和反向传播的梯度误差进行卷积，就得到了前一次的梯度误差。这个例子直观的介绍了为什么对含有卷积的式子反向传播时，卷积核要翻转180度的原因。
然后不用算$\frac{\partial a^{l-1}}{\partial z^{l-1}}$了吗？

### 3.1 已知卷积层的$\delta^{l}$，推导该层的$W, b$的梯度
卷积层$z$和$W, b$的关系为：
$$ 
z^{l}=a^{l-1} * W^{l}+b
 $$
因此我们有：
$$ 
\frac{\partial C}{\partial W^{l}}=\frac{\partial C}{\partial z^{l}} \frac{\partial z^{l}}{\partial W^{l}}=a^{l-1} * \delta^{l}
 $$

假设我们输入$a$是$4 \times 4$的矩阵，卷积核$W$是$3 \times 3$的矩阵，输出$z$是$2 \times 2$的矩阵,那么反向传播的$z$的梯度误差$\delta$也是$2 \times 2$的矩阵。
$$ 
\frac{\partial J(W, b)}{\partial W^{l}}=\left( \begin{array}{llll}{a_{11}} & {a_{12}} & {a_{13}} & {a_{14}} \\ {a_{21}} & {a_{22}} & {a_{23}} & {a_{24}} \\ {a_{31}} & {a_{32}} & {a_{33}} & {a_{34}} \\ {a_{41}} & {a_{42}} & {a_{43}} & {a_{44}}\end{array}\right) * \left( \begin{array}{cc}{\delta_{11}} & {\delta_{12}} \\ {\delta_{21}} & {\delta_{22}}\end{array}\right)
 $$

对于b，通常的做法是将$\delta^{l}$的各个子矩阵的项分别求和，得到一个误差向量，即为$b$的梯度：
$$ 
\frac{\partial J(W, b)}{\partial b^{l}}=\sum_{u, v}\left(\delta^{l}\right)_{u, v}
 $$

### 3.2 已知池化层的$\delta^{l}$，推导上一隐藏层的$\delta^{l-1}$
在前向传播算法时，池化层一般我们会用MAX或者Average对输入进行池化，池化的区域大小已知。现在我们反过来，要从缩小后的误差$\delta^{l}$，还原前一次较大区域对应的误差。

#### upsampling
在反向传播时，我们首先会把$\delta^{l}$的所有子矩阵矩阵大小还原成池化之前的大小，然后如果是MAX，则把$\delta^{l}$的所有子矩阵的各个池化局域的值放在之前做前向传播算法得到最大值的位置。如果是Average，则把$\delta^{l}$的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置。这个过程一般叫做upsample。

假设我们的池化区域大小是$2 \times 2$，$\delta^{l}$的第k个子矩阵为:
$$ 
\delta_{k}^{l}=\left( \begin{array}{ll}{2} & {8} \\ {4} & {6}\end{array}\right)
 $$
由于池化区域为$2 \times 2$，我们先还原$\delta_{k}^{l}$，即变成：
$$ 
\left( \begin{array}{llll}{0} & {0} & {0} & {0} \\ {0} & {2} & {8} & {0} \\ {0} & {4} & {6} & {0} \\ {0} & {0} & {0} & {0}\end{array}\right)
 $$
 如果是MAX，假设我们之前在前向传播时记录的最大值位置分别是左上，右下，右上，左下，则转换后的矩阵为：
 $$ 
\left( \begin{array}{llll}{2} & {0} & {0} & {0} \\ {0} & {0} & {0} & {8} \\ {0} & {4} & {0} & {0} \\ {0} & {0} & {6} & {0}\end{array}\right)
 $$

如果是Average，则进行平均：转换后的矩阵为：
$$ 
\left( \begin{array}{cccc}{0.5} & {0.5} & {2} & {2} \\ {0.5} & {0.5} & {2} & {2} \\ {1} & {1} & {1.5} & {1.5} \\ {1} & {1} & {1.5} & {1.5}\end{array}\right)
 $$
这样我们就得到了上一层$\frac{\partial J(W, b)}{\partial a_{k}^{l-1}}$的值，要得到$\delta_{k}^{l-1}$：
$$ 
\delta_{k}^{l-1}=\frac{\partial J(W, b)}{\partial a_{k}^{l-1}} \frac{\partial a_{k}^{l-1}}{\partial z_{k}^{l-1}}=u p s a m p l e\left(\delta_{k}^{l}\right) \odot \sigma^{\prime}\left(z_{k}^{l-1}\right)
 $$
其中，upsample函数完成了池化误差矩阵放大与误差重新分配的逻辑。
对于张量$\delta^{l-1}$，我们有：
$$ 
\delta^{l-1}=u p s a m p l e\left(\delta^{l}\right) \odot \sigma^{\prime}\left(z^{l-1}\right)
 $$
