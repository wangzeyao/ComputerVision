# 常用梯度下降方法

## 1. 随机梯度下降法SGD

由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。随机梯度下降法（Stochastic Gradient Descent，简称SGD）正是为了解决批量梯度下降法这一弊端而提出的。
$$ 
\begin{array}{l}{J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \frac{1}{2}\left(y^{i}-h_{\theta}\left(x^{i}\right)\right)^{2}=\frac{1}{m} \sum_{i=1}^{m} \cos t\left(\theta,\left(x^{i}, y^{i}\right)\right)} \\ {\cos t\left(\theta,\left(x^{i}, y^{i}\right)\right)=\frac{1}{2}\left(y^{i}-h_{\theta}\left(x^{i}\right)\right)^{2}}\end{array}
 $$
 
 随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。

　　**优点**：训练速度快；

　　**缺点**：准确度下降，并不是全局最优；不易于并行实现。

## 2. 小批量梯度下降法MBGD
有上述的两种梯度下降法可以看出，其各自均有优缺点，那么能不能在两种方法的性能之间取得一个折衷呢？即，算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法（Mini-batch Gradient Descent，简称MBGD）的初衷。

　　MBGD在每次更新参数时使用b个样本（b一般为10）

## 3.momentum 动量法
![enter image description here](https://lh3.googleusercontent.com/QvgTjSuiQc9Q8w-eyorPZpAweNXM6Gl4sT7Zq3shYB29JFjXx9iiLR4v_4scUwv3QKjAJ9Jq4of_)
A为起始点，首先计算A点的梯度$\nabla a$，然后到B点。
$$ 
\theta_{n e w}=\theta-\alpha \nabla a
 $$
 到了B点需要加上A点的梯度，这里梯度需要有一个衰减值γ。这样的做法可以让早期的梯度对当前梯度的影响越来越小，如果没有衰减值，模型往往会震荡难以收敛，甚至发散。所以B点的参数更新公式是这样的：
 $$ 
\begin{array}{c}{v_{t}=\gamma v_{t-1}+\alpha \nabla b} \\ {\theta_{n e w}=\theta-v_{t}}\end{array}
 $$
 其中$v_{t-1}$示之前所有步骤所累积的动量和。  
这样一步一步下去，带着初速度的小球就会极速的奔向谷底。

## 4.AdaGrad算法
介绍AdaGrad算法，它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。
AdaGrad算法会使用一个小批量随机梯度$g_{t}$按元素平方的累加变量$\mathcal{S}_{t}$，然后根据$\mathcal{S}_{t}$的大小来确定学习率。在时间步0，AdaGrad将$\mathcal{S}_{0}$中每个元素初始化为0。在时间步$t$首先将小批量随机梯度$g_{t}$按元素平方后累加到变量$\mathcal{S}_{t}$
$$ 
s_{t} \leftarrow s_{t-1}+g_{t} \odot g_{t}
 $$
 其中$(0)$是按元素相乘。接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：
 $$ 
\boldsymbol{x}_{t} \leftarrow \boldsymbol{x}_{t-1}-\frac{\eta}{\sqrt{s_{t}+\epsilon}} \odot g_{t}
 $$
 其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$
 这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。
 小批量随机梯度按元素平方的累加变量$\boldsymbol{S}_{t}$出现在学习率的分母项中。因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。
 **缺点**:当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。
## 5.RMSProp算法
不同于AdaGrad算法里状态变量$\boldsymbol{S}_{t}$是截至时间步t所有小批量随机梯度$g_{t}$按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。具体来说，给定超参数$0 \leq \gamma<1$，RMSProp算法在时间步$t>0$计算
$$ 
s_{t} \leftarrow \gamma s_{t-1}+(1-\gamma) g_{t} \odot g_{t}
 $$
 和AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量
 $$ 
\boldsymbol{x}_{t} \leftarrow \boldsymbol{x}_{t-1}-\frac{\eta}{\sqrt{s_{t}+\epsilon}} \odot \boldsymbol{g}_{t}
 $$
RMSProp算法不是像AdaGrad算法那样暴力直接的累加平方梯度，而是加了一个衰减系数来控制历史信息的获取多少。[指数加权平均](https://zhuanlan.zhihu.com/p/29895933)
因为RMSProp算法的状态变量$\boldsymbol{S}_{t}$是对平方项$\boldsymbol{g}_{t} \odot \boldsymbol{g}_{t}$的指数加权移动平均，如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变。

## 6.AdaDelta算法
AdaDelta算法也像RMSProp算法一样，使用了小批量随机梯度$g_{t}$按元素平方的指数加权移动平均变量$\boldsymbol{S}_{t}$。在时间步0，它的所有元素被初始化为0。给定超参数$0 \leq \rho<1$在时间步$t>0$，同RMSProp算法一样计算
$$ 
\boldsymbol{s}_{t} \leftarrow \rho \boldsymbol{s}_{t-1}+(1-\rho) \boldsymbol{g}_{t} \odot \boldsymbol{g}_{t}
 $$
与RMSProp算法不同的是，AdaDelta算法还维护一个额外的状态变量$\Delta \boldsymbol{x}_{t}$，其元素同样在时间步0时被初始化为0。我们使用$\Delta \boldsymbol{x}_{t-1}$来计算自变量的变化量：
$$ 
\boldsymbol{g}_{t}^{\prime} \leftarrow \sqrt{\frac{\Delta \boldsymbol{x}_{t-1}+\epsilon}{s_{t}+\epsilon}} \odot \boldsymbol{g}_{t}
 $$
 接着更新自变量：
 $$ 
\boldsymbol{x}_{t} \leftarrow \boldsymbol{x}_{t-1}-\boldsymbol{g}_{t}^{\prime}
 $$
 最后，我们使用$\Delta \boldsymbol{x}_{t}$来记录自变量变化量$\boldsymbol{g}_{t}^{\prime}$按元素平方的指数加权移动平均：
 $$ 
\Delta \boldsymbol{x}_{t} \leftarrow \rho \Delta \boldsymbol{x}_{t-1}+(1-\rho) \boldsymbol{g}_{t}^{\prime} \odot \boldsymbol{g}_{t}^{\prime}
 $$
可以看到，如不考虑$\epsilon$的影响，AdaDelta算法跟RMSProp算法的不同之处在于使用$\sqrt{\Delta x_{t-1}}$来替代超参数$\eta$

## 7.Adam算法
Adam算法使用了
### 1.动量变量$v_{t}$
给定超参数$0 \leq \beta_{1}<1$，时间步t的动量变量$v_{t}$即小批量随机梯度$g_{t}$的指数加权移动平均：
$$ 
\boldsymbol{v}_{t} \leftarrow \beta_{1} \boldsymbol{v}_{t-1}+\left(1-\beta_{1}\right) \boldsymbol{g}_{t}
 $$
### 2.RMSProp算法中的$\mathcal{S}_{t}$
给定超参数$0 \leq \beta_{2}<1$， 将小批量随机梯度按元素平方后的项$\boldsymbol{g}_{t} \odot \boldsymbol{g}_{t}$数加权移动平均得到$\mathcal{S}_{t}$：
$$ 
s_{t} \leftarrow \beta_{2} s_{t-1}+\left(1-\beta_{2}\right) g_{t} \odot g_{t}
 $$
 由于我们将$v_{0}$和$\mathcal{S}_{0}$中的元素都初始化为0， 在时间步$t$我们得到$\boldsymbol{v}_{t}=\left(1-\beta_{1}\right) \sum_{i=1}^{t} \beta_{1}^{t-i} \boldsymbol{g}_{i}$。将过去各时间步小批量随机梯度的权值相加，得到$\left(1-\beta_{1}\right) \sum_{i=1}^{t} \beta_{1}^{t-i}=1-\beta_{1}^{t}$  
1(1−β1)∑i=1tβ1t−i=1−β1t。需要注意的是，当$t$较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当$\beta_{1}=0.9$时，$\boldsymbol{v}_{1}=0.1 \boldsymbol{g}_{1}$。
为了消除这样的影响，对于任意时间步$t$，我们可以将$v_{t}$再除以$1-\beta_{1}^{t}$从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，我们对变量$v_{t$和$\mathcal{S}_{t}$均作偏差修正：
$$ 
\begin{array}{c}{\hat{\boldsymbol{v}}_{t} \leftarrow \frac{\boldsymbol{v}_{t}}{1-\beta_{1}^{t}}} \\ {\hat{\boldsymbol{s}}_{t} \leftarrow \frac{\boldsymbol{s}_{t}}{1-\beta_{2}^{t}}}\end{array}
 $$
 接下来，Adam算法使用以上偏差修正后的变量$\hat{v}_{t}$和$\hat{\boldsymbol{s}}_{t}$，将模型参数中每个元素的学习率通过按元素运算重新调整：
 $$ 
\boldsymbol{g}_{t}^{\prime} \leftarrow \frac{\eta \hat{\boldsymbol{v}}_{t}}{\sqrt{\hat{\boldsymbol{s}}_{t}}+\epsilon}
 $$
 最后，使用$g_{t}^{\prime}$迭代自变量：
 $$ 
\boldsymbol{x}_{t} \leftarrow \boldsymbol{x}_{t-1}-\boldsymbol{g}_{t}^{\prime}
 $$
